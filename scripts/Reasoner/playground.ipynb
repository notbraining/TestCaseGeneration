{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b728e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from typing import cast\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from datasets.load import Dataset, DatasetDict\n",
    "from verl.utils.hdfs_io import copy, makedirs\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe704735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"BAAI/TACO\"\n",
    "dataset: DatasetDict = cast(DatasetDict, datasets.load_dataset(data_source, \"ALL\"))\n",
    "train_dataset: Dataset = dataset[\"train\"]\n",
    "test_dataset: Dataset = dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcbf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_format(question: str, inputs: list[str], outputs: list[str]):\n",
    "    prompts = []\n",
    "    ground_truth = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        if (isinstance(input, str) and isinstance(output, str)):\n",
    "            query = question + \"\\n\" + input\n",
    "            answer = output\n",
    "            prompts.append(query)\n",
    "            ground_truth.append(answer)\n",
    "    return prompts, ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48064cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25443/25443 [00:40<00:00, 627.85it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = []\n",
    "ground_truth = []\n",
    "for sample in tqdm(test_dataset):\n",
    "    try:\n",
    "        question = sample['question']\n",
    "        inputs = json.loads(sample['input_output'])['inputs']\n",
    "        outputs = json.loads(sample['input_output'])['outputs']\n",
    "        a, b = make_format(question, inputs, outputs)\n",
    "        prompts.extend(a)\n",
    "        ground_truth.extend(b)\n",
    "    except:\n",
    "        continue\n",
    "assert len(prompts) == len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f60c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = [\"BAAI/TACO\"] * len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584c5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt : str) -> list[dict]:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"answer or u die :)\" \n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "prompt = list(map(format_prompt, prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2507cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ability = [\"code\"] * len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86fce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_model(input_output_pair : tuple[str, str]) -> dict:\n",
    "    assert isinstance(input_output_pair[0], str)\n",
    "    assert isinstance(input_output_pair[1], str)\n",
    "    return {\n",
    "        \"style\": \"rule\",\n",
    "        \"ground_truth\": {\"inputs\": input_output_pair[0], \"outputs\": input_output_pair[1]},\n",
    "    }\n",
    "reward_model = list(map(format_reward_model, zip(prompts, ground_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bdcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"data_source\" : data_source,\n",
    "    \"prompt\": prompt,\n",
    "    \"ability\": ability,\n",
    "    \"reward_model\": reward_model\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1309dd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_source</th>\n",
       "      <th>prompt</th>\n",
       "      <th>ability</th>\n",
       "      <th>reward_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/TACO</td>\n",
       "      <td>[{'role': 'system', 'content': 'answer or u di...</td>\n",
       "      <td>code</td>\n",
       "      <td>{'style': 'rule', 'ground_truth': {'inputs': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI/TACO</td>\n",
       "      <td>[{'role': 'system', 'content': 'answer or u di...</td>\n",
       "      <td>code</td>\n",
       "      <td>{'style': 'rule', 'ground_truth': {'inputs': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI/TACO</td>\n",
       "      <td>[{'role': 'system', 'content': 'answer or u di...</td>\n",
       "      <td>code</td>\n",
       "      <td>{'style': 'rule', 'ground_truth': {'inputs': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/TACO</td>\n",
       "      <td>[{'role': 'system', 'content': 'answer or u di...</td>\n",
       "      <td>code</td>\n",
       "      <td>{'style': 'rule', 'ground_truth': {'inputs': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI/TACO</td>\n",
       "      <td>[{'role': 'system', 'content': 'answer or u di...</td>\n",
       "      <td>code</td>\n",
       "      <td>{'style': 'rule', 'ground_truth': {'inputs': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_source                                             prompt ability  \\\n",
       "0   BAAI/TACO  [{'role': 'system', 'content': 'answer or u di...    code   \n",
       "1   BAAI/TACO  [{'role': 'system', 'content': 'answer or u di...    code   \n",
       "2   BAAI/TACO  [{'role': 'system', 'content': 'answer or u di...    code   \n",
       "3   BAAI/TACO  [{'role': 'system', 'content': 'answer or u di...    code   \n",
       "4   BAAI/TACO  [{'role': 'system', 'content': 'answer or u di...    code   \n",
       "\n",
       "                                        reward_model  \n",
       "0  {'style': 'rule', 'ground_truth': {'inputs': '...  \n",
       "1  {'style': 'rule', 'ground_truth': {'inputs': '...  \n",
       "2  {'style': 'rule', 'ground_truth': {'inputs': '...  \n",
       "3  {'style': 'rule', 'ground_truth': {'inputs': '...  \n",
       "4  {'style': 'rule', 'ground_truth': {'inputs': '...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c88e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de929b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"TACO_test_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dd159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e3d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competitiveLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
